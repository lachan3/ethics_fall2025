---
title: 'Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle'
date: 2025-10-11
permalink: /posts/2025/09/blog-post-7/
tags:
  - Case Study
  - AI
  - Ethics
---

My reaction to a case study of how South Asian cultural representations can be generalized to an outsiders gaze from limited training with South Asian culture being shaped by global power. 

**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

Summary
---
South Asian cultural representations can be generalized to an outsiders gaze from limited training with South Asian culture being shaped by global power. 

Discussion Questions
---
1. What does cultural representation mean to you, and how might this definition and your experience with past representation impact how you would evaluate representation of your identity in generative AI models? What aspects of your identity do you think you would center when evaluating representations in AI model output?

I grew up with a lot of family members being adopted, like my dad. There were adoptions from two colombian kids from my grandparents, so this brought a whole new culture into our family because we were white. It is important to educate, have accurate and diverse cultural representation so there are no harmful stereotypes that can fester into real life situations like getting a job. AI can be bad with holding biases that score less with minorites. If I wanted generative AI to have an accuratae representation of myself I would say it would need to get my look right, skin color, face shape, body. It would also need to get down my beliefs, manner, and morals.

2. What do you think is the role of small-scale qualitative evaluations for building more ethical generative AI models? How do they compare to larger, quantitative, benchmark-style evaluations?

Small-scale qualitative evaluations for building more ethical generative AI is just bad when compared to larger, quantitative, benchmark-style evaluations. Smaller evulations on data sets leads to less accurate data when compared to larger, more diverse groups.

3. Participants in this study shared “aspirations” for generative AI to be more inclusive, but also noted the tensions around making it more inclusive. Do you think AI can be made more globally inclusive?

4. What mitigations do you think developers could explore to respond to some of the concerns raised by participants in this study?

Force larger inclusive training for AI.

5. As mentioned in this case study, representation is not static; it changes over time, is culturally situated, and varies greatly from person to person. How can we “encode” this contextual and changing nature of representation into models, datasets, and algorithms? Is the idea of encoding at odds with the dynamism and fluidity of representation?

We could just have an addition to the training, every year or so. However this would not account for lost culture.

6. How can we learn from the history of technology and media to build more responsible and representative AI models?

Spend more time on training and prep work before releasing the AI for use in the public.

New Question
---

Is it possible to not do bruh?

I chose this question because

Reflection
---

Reading this case study made me think about my childhood and spending a little too much time on video games. I don't think this made me underdeveloped socially, but I could have spent more time developing other, more useful skills. When and if I have children I want to make sure they do not spend too much time with social media, games, and AI. I am assuming that AI will be used a lot more when I am older.