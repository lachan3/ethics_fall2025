---
title: 'Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle'
date: 2025-10-11
permalink: /posts/2025/09/blog-post-7/
tags:
  - Case Study
  - Generative AI
  - Ethics
---

My reaction to a case study of how South Asian cultural representations can be generalized by generative AI to an outsider's western gaze from limited training with South Asian culture.

**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

Summary
---
The case study discusses how South Asian cultural representations can be generalized to an outsider's gaze from limited training in the lens of western ideals. It mentions specific AI models that produce images that are either not up to date or just inaccurate representations of countries and cultures like Bangladeshi and Pakistani.


Cultural Representation
---
The following are my answers to various questions asked at the end of the case study.

Cultural representation is a newer idea to me. I have never had to think about representing my culture,because I have been deep in it and have never really stepped outside of it. I grew up with a lot of family members being adopted, like my dad. There were adoptions from two Colombian kids from my grandparents, so this brought a whole new culture into our family because we were white. It is important to educate, have accurate and diverse cultural representation so there are no harmful stereotypes that can fester into real life situations like getting a job. AI can be bad at holding biases that score less with minorities. If I wanted generative AI to have an accurate representation of myself I would say it would need to get my look right, skin color, face shape, body. It would also need to get down my beliefs, manners, and morals.

There are distinct roles between the use of small-scale qualitative evaluations for building more ethical generative AI models. Small scale qualitative evaluations should be used in context specific evaluations for figuring out if something is unethical. Interviews can also be used to get more diverse opinions on it. "Ultimately, participant aspirations for generative AI focused heavily on restoring agency and community control over the terms of representation" [Read more](https://mit-serc.pubpub.org/pub/bfw5tscj#nyx1jxwa3mc). However, the smaller evaluations are not broad and are specific issues. Larger, quantitative, benchmark-style evaluations are good for seeing how much biases occur in broader areas, and/or over time. They can be more efficent if done correctly. I think it's good to use a combination of the two, to ensure accurate representations of many cultures.

Participants in this study shared “aspirations” for generative AI to be more inclusive, but also noted the tensions around making it more inclusive. I believe, if done right, that AI can be made more globally inclusive. There are a lot of problems and issues to look out for. "Left unchecked, algorithmic systems can scale existing regimes of representation through patterns of over- and underrepresentation in algorithmic systems" [Read more](https://mit-serc.pubpub.org/pub/bfw5tscj#nzxy9iyi0ua). There needs to be more development of AI reasoning in the context of other cultures, this was mentioned by one of the participants in the case study. “why can’t we imagine a more authentic world that our communities can build ourselves?” (P32, Pakistan). Maybe each country can contribute to a globally inclusive AI. This way, it will get most if not all different cultures.

Like I mentioned above there could be leaders from different communities, cultures, and countries that are involved in the design and evaluation of generative AI models. Developers could also use regional institutions like universites, government, boards or NGOs to adapt base models to their own cultural contexts, and use cultural evaluation frameworks that are both qualitative and quantitative. They would involve the regional institutions into the development process of the AI, by having them look through what data is accurate, and having them run and curate benchmarks alongside developers. Also, representation is dynamic and socially constructed, inclusivity must be treated as an ongoing process, and these AI models should be regularly retrained and reviewed as culture evolves.

As mentioned in this case study, representation is not static; it changes over time, is culturally situated, and varies greatly from person to person. How we “encode” this contextual and changing nature of representation into models, datasets, and algorithms is important. The AI design would have to allow for iterative implementation. Datasets could be treated as living archives that change through communities reviews, and documentation of when and how data was collected. Models can use feedback systems that allow users to flag outdated or harmful representations, prompting regular retraining that accounts for shifting social norms and values.

It would be wise to spend more time on training and prep work before releasing the AI into the public.
We can learn from the history of technology and media to build more responsible and representative AI models. On two separate occasions Twitter has released AI chat bots that act like users, both times they had spout racist, and sexist remarks. Developers of AI should also implement cultural technologies, meaning using historical awareness with the AI pipeline, with questioning who creates datasets, and which cultural narratives are amplified or erased.

Something to Consider
---

Will there ever be a point in which AI has an accuracy of the representation of all cultures, that is good enough to the point where it outweighs the harm? Who gets to make that decision? Can we, ourselves, ever achieve a complete unbiased state?

I ask this question because I was thinking about how AI will always have a margin of error. At what  rate of the success/accuracy of AI outputs benefit our society. Also who gets to say when it is too harmful or helpful? How might that impact how much leniency they have with the decision? With that being said we ourselves are far from perfect. I am curious how combining our biases with inaccurate AIs can further exacerbate these problems. 

Reflection
---

Reading this case study made me think about how little I think about problems like misrepresentation. This is probably due to me being a part of western culture. I am also white so that further pushes this thought. I don't get directly affected by it, because in a way I am apart of the beneficiaries of these AI biases. 