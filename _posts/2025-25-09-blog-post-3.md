---
title: 'ALG Blog Post 1: Exceptions to a Data Driven Rule'
date: 2025-09-25
permalink: /posts/2025/09/blog-post-3/
tags:
  - Case Study
  - Algorithms
  - Ethics
  - Data
---

My response to an case study that discusses the ethical problems of data driven algorithms.

**Case Study:**  
[The Right to Be an Exception to a Data-Driven Rule](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)


Summary
---
The case study talks about how biases can creep into data driven algorithms, and allow them to make decisions that are rooted in stereotypes. The case study argues that data driven algorithms can be useful. However this requires that data driven algorithms are certain only if the levels of individualization and certainty are high enough to justify the level of harm that would result from that recommendation.


Data Driven Rules
---
The following are my answers to various questions asked at the end of the case study.

Data driven rules are guidelines built into complex sorting algorithms. But what does it mean to be a data-driven exception? Is an exception the same as an error? Data driven exceptions are if an element that is sorted, is put into an incorrect group. Exceptions are wrong but not errors. Humans decide what is and is not an wrong based on caveats. These algorithms are not human in the sense that they cannot apply these exceptions to the sorting algorithms.

There are a lot of other factors that can differentiate data-driven decisions from human ones. One being that humans use context and slowly move through data. Algorithms are fast and don't look at each context that corresponds with that data point/set, they instead match and sort. Because algorithms look at large data sets and do not look at each case deeply, the unwanted sorting can be amplified. Humans also have their own biases when looking at data. This is why it is more wise to have a diverse group of people that look at data sets and interpret them, thus reducing the amount of biases. Often, these algorithms consist of a singular AI that has its own biases, and those biases, harmful or not, are imposed.

Individualization can have both negative and positive side effects. Individualization can narrow down sorting algorithms consideration with more context, and can avoid unfair generalizations. This can allow the algorithm to make more well rounded decisions with increased accuracy. Also if the use case is something small like a music recommendation algorithm, it can make the user experience more enjoyable. This however can risk your privacy, by requiring more data. Also, if individualization relies too much on personal information, it might unintentionally learn and reinforce sensitive patterns, like race, gender, thus increasing bias rather than reducing it.

"Many worry that a machine decision simply cannot incorporate the same nuance and values that a human could" [Read more about values](https://mit-serc.pubpub.org/pub/right-to-be-exception#nn8oilwejfa).

When the stakes are high (e.g., in criminal sentencing), is there any evaluation metric (e.g., accuracy) that can justify the use of a data-driven rule without the consideration of uncertainty? Uncertainty is crucial, because if you have someone that is being sentenced to death, depending on the uncertainty, they could be killed unjustifiably. There is no one metric to justify the use of a data driven rules, because of the massive number of incalculable variables. I believe it is best to use almost no AI in serious situations like the death penalty, and instead many people from diverse backgrounds.


Something to Consider
---
Is it ethical to use data driven rules when the uncertainty is high but the stakes are low, like music recommendations?

I chose this question because I find music recommendations very helpful and I don't see a problem with it. In fact the uncertainty can sometimes show me music That I would have never sought out. However there may be concerns for privacy and data collection, especially if you are using a lot of low stakes algorithms that collect your data and contribute to an overall less private digital footprint.


Reflection
---
Reading through this case study made me think about certainty and how it is unethical to be uncertain when taking action that could result in harm. In a perfect world you can be a law abiding citizen and not have to worry about data driven algorithms making incorrect sorts, or judgements when you are trying to get a loan or a job. From what I have gathered it seems to me that these can be used in cases where the harm does not outweigh the potential for good. This would not include loans, job hiring, and college applications systems.
