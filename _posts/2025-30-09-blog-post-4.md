---
title: 'ALG Blog Post 2: How Generative AI Works and How It Fails'
date: 2025-09-30
permalink: /posts/2025/09/blog-post-4/
tags:
  - Case Study
  - Generative AI
  - Ethics
---

Ways in which AI generates content. Ways generative AI fails and causes problems

**Case Study:**  
[How Generative AI Works and How It Fails](https://mit-serc.pubpub.org/pub/f3o5mpn6/release/1?readingCollection=3a6c54f1)

Summary
---
The purpose of the case study is to explain the various ways AI generates content like images and text with diffusion and transformer models, as well as their problems and limitations. Another purpose is to highlight the societal problems caused by generative AI, like misinformation, deepfakes, and labor exploitation.

The Use of Creative Work for Training
---
The most utilized and popular AI models use stolen and pirated data to generate content that is often passed as its own. These companies do not attribute to the creators they trained off of, while making money off of that same AI. Creative jobs are at risk because generative AI can replicate and mass produce creative products.

AI at this point in time does not understand what it is doing, it lacks genuine understanding, consciousness, and the ability to reason or make moral judgment. "Philosopher Harry Frankfurt defined bullshit as speech that is intended to persuade without regard for the truth. In this sense, chatbots are bullshitters. They are trained to produce plausible text, not true statements" (https://mit-serc.pubpub.org/pub/f3o5mpn6#ncd1ih09488).

While AI uses different methods to produce video and images with diffusion, it still does not know what it is outputing, in a sense it is still bullshitting. This also is an insult to art, and artists because it neglects the meaning and time commitment of art.

So, how can those who want to change the system go about doing so? We can start with the people most affected by generative AI "art", the real Artists. They can take legal action and talk to politicians and lawmakers in large groups. They could try to replicate their art with the AIs to show how it can copy them. They could also data poison their own content to prevent their content being trained on by AI.

What can solve this problem? Could the market solve it through licensing agreements between publishers and AI companies? The market has already been benefiting from AI, this does not make it right or ethical, because large companies can replace their low paid workers with automated AI. However, AI companies could scrap existing models and begin to make data sets that are made up of data that have permission and/or pay to use. Although, this is not realistic because of it costing a lot to redo the training. Also, there would need to be governmental orders to disband current AI models.

What about copyright law, either interpreting existing law or by updating it? There should be transparency requirements for companies to disclose training data sources. AI companies should be required to pay people whose content they used to train the AI.

Other helpful polices could come from the United Nations, with them creating global laws about AI because it trains on data used from all around the world. However I believe this would require all the big players in the world of AI, who are not in the UN like China to agree to these terms. China could lead AI if we limit AI with legislation, instead of taking precedence over it.


New Question
---

Deepfakes are becoming easier to create and harder to detect. Should society focus more on developing  detection tools, or passing stronger legislation?

I chose this question, because I have been seeing stories of kids in school making deepfakes of teachers doing/saying bad things. There are also a lot of deepfakes of politicians saying bad things. In the comment sections of these posts there are handfuls of people believing deepfakes. I think it is important to discuss the harm these deepfakes can cause to society. Because they are becoming so easy to make, I think there should be legislation passed to prevent bad actors.

Reflection
---

I already knew most of what the case study was referring to. Although I have not thought about the legality of AI as much as before. I remember watching a video about how Facebook exploits out sourced labour for weeding out toxic content. The video talks about the mental decline of these workers from  them seeing vile content. There are stories of these workers committing suicide, this can be attributed to the lack of benefits like mental health counseling. I think the exploitation done by these AI companies (while not as bad) can be connected to these past exploits. There needs to be more support for these workers with benefits and better pay.