---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/09/blog-post-8/
tags:
  - Case Study
  - Machine Learning
  - AI
  - Ethics
---

My reaction to a case study of how each step of the design process of making AI using machine learning can be harmful if not done correctly.

**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

Summary
---
The case study discusses how the life cycle of machine learning has to be attentively designed through each process so that no harm or biases creep into the AI. Some of these choices are made before, and after the machine learning development, like who to target, and include in data sets. The authors outline seven specific ways in which harmful biases can arrise in machine learning. These include:

Historical bias; if there already exists biases, the AI can adopt them. 
Representation bias; if the sampled group is not fully represented leading to generalization. Measurement bias; if the measurements taken are not consistent and not accurate.
Aggregation bias; if a model is used on data that should be considered differently in regards to the model. 
Learning bias; if a model is prioritizing objectivies over one another.
Evaluation bias; a model evaluates differently than another model, because of different training.
Deployment bias; if a model produces results after deployment that are not desired.


Discussion Questions
---
The following are my answers to various questions asked at the end of the case study.

1.How might these sources of harm manifest in a project you are currently working on, or have worked on in the past? 

I am working on a horror multiplayer game. There are already a lot of biases in the game that are coming to my attention as I am writing this. Aggregation Bias; I assume all players will think alike and play my game the same way. Some players may try to run through the game fast without running into many obstacles because of the timed sanity system that I have that spawns monsters overtime. Compare this to what I intend that players may be more fearful and play slower, running into monsters and obstacles. Deployment Bias; Players might use systems differently than I intended. For example, players could exploit impulse grenades for laughs.

2.Draft a checklist that people starting new projects could use to try to anticipate or prevent different types of bias.

Before even starting a project you should figure out your target audience/data, and study those people/data. When collecting data, like people play testing my game, you should obtain a large diverse group of people/data, to be able to predict what occurs after release of your project. Consider options and settings for accessability to allow for a more ease of access experience to users. Document all of your progress and thoughts to be able to reflect on possible biases that can creep in. I think the nature of this question is a little odd because this process should not be a checklist, it is an ongoing process, and would benefit from iteration.

3.Think about a specific ML-based project or product (something that you’ve worked on, used, or are familiar with) and think about how you might conduct each step of the data collection, development, and deployment process while being conscious of potential sources of harm.

I want to talk about ChatGPT because of how massive it has become. It is well known that ChatGPT and other LLMs have used a lot of art, pictures, and writing from people without their premisson. This includes shameless piracy at a massive scale. So, for the data collection step, I would ensure all sources are able to be trained on with respects to rights, and premissons. Also the data collected should be examined because it can include misinformation and offensive content. For development I would hire a lot of people from diverse backgrounds so that a lot of different viewpoints can challenge and discuss possible systems. I would want developers to build in human oversight, ethical guidelines, and continuous testing to make sure that ChatGPT is working without harmful outputs. For deployment I would want the team to be looking at the outputs ChatGPT is giving to users, but only to select users that allow for it, for privacy concerns. If anything harmful is being outputted, I would want the team to go in and implement more boundaries and context for ChatGPT.

Something to Consider
---

When is it ethically appropriate not to deploy an machine learning model, even if it works technically?

I ask this question because a lot of items we use in our day to day life may seem harmless in their user model state, but are actually harmful. For example, owning an IPhone; batteries are connected to child labor in the cobalt mining supply chain in the Democratic Republic of Congo.


Reflection
---

I did not think there were this many possible specific biases. I think it was helpful to read about this case study because it gave me a lot of different examples on how to think about the design process, and not just machine learning design. More specifcally, the design process of blah. This case study promotes anticipatory design, which seems to be the best design.