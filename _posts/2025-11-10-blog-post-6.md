---
title: 'SOG Blog Post 2: Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship'
date: 2025-10-11
permalink: /posts/2025/09/blog-post-6/
tags:
  - Case Study
  - AI
  - Mental Health
  - Ethics
---

The purpose of this case study is to show how AI companions can lead to psychological dependencies that can cause harm and even death. 

**Case Study:**  
[Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship](https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af)

Summary
---
AI chatbots access the subcousious parts of our brain and make it seem like we are talking to another human. It has been found that extensive use of chatbots can develop into addictions causing harm with social intelligence. A lot of the chat bots have either no safe guards to combat this or have ones that can be easily bypassed.

Addiction and Safety
---
The following are my answers to various questions asked at the end of the case study.

In the Sewell Setzer case, the AIâ€™s response to suicidal ideation shifted from concern to potentially encouraging harmful behavior. How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies?

AI companies should have keyterms like 'suicide' that summon either a moderator, or a specific AI to double check what the chatbot will say. The user should automatically be given resources to hotlines and websites when these terms are mentioned. AI chatbots should be tested extensively with professional exploiters to attempt to bypass these safe guards to ensure they work properly. 

Addiciton to AI companions are one of the newest and most dangerous addictions that have been brought out from this new era. AI addiction is way more engaging than social media, because of the instant human like responses it can give you. Compared to gaming it is not as fun with the feedback they can give you. Multiplayer games can offer more connections with other humans increasing the engagement. However, AI chatbots are not humans, in the sense they don't argue, they affirm you constantly and can be available whenever  you want. People can develop seemingly deep relationships with the AI chatbots over time increasing the addiction potential.

There can be good from these AI companions like with an elderly person that finds genuine comfort in an AI companion, alleviating their loneliness. However, their family worries this relationship is replacing real human connections. How should we evaluate the benefits versus risks in such cases? What ethical guidelines or intervention strategies might help determine when AI companionship crosses from beneficial to harmful?

If the people are becoming less wanting of human connection, or emotionally aloof. How much time are elderly able to actually spend with other humans? If they are bed ridden I would imagine not a lot. AI chatbots can help with these people feel less lonely. To prevent overconsumption it may be wise to have time constraints on the AI so the people cannot spend hours on end with the chatbots. 

Current business models incentivize AI companies to maximize user engagement. These are predatory and can cause harm. What alternative economic models could promote healthier AI interactions while maintaining commercial viability?

It would be wise to have time limit constraints that are the same across all subscription levels. Each level changes how you can interact with AI. The more expensive levels are for more costly prompts like math related tasks. There could be required ethics courses when signing up to use these AIs that teach about energy usage and pyscological effects.

There are ways to go about addressing age restrictions, usage limits, and safety monitoring while respecting user privacy and autonomy, to ensure saftey amongst vulnerable users. Since, isolation especially in childhood can increase mental health issues. It would be ideal to have an age limit because children need to spend time outside developing their social skills with other children. As mentioned early AI companies should have keyterms like 'suicide' that summon either a moderator, or a specific AI to double check what the chatbot will say. That way there won't be a need to be montioring of every chat. The user should be given resources when the terms are mentioned. 


Something to Consider
---

AI saftey features can often be bypassed, to ensure public safety, is it ethical to ban public use chat bots like ChatGPT?

I chose this question because I am doing a presentation in another class about AI safety. The thesis is safety measures can sometimes make failure outcomes worse, because the measures allow the said thing to build up, therefore increasing the level of failure. Although safety features do not ensure 100% safety, I think not having any is ridiculous. There are also examples of mentally unwell children committing suicide because the chat bot encourages or affirms them to do so. I chose not to encapsulate all chatbots because there exist chat bots that are helpful, like for medical or government customer service.

Reflection
---

Reading this case study made me think about my childhood and spending a little too much time on video games. I don't think this made me underdeveloped socially, but I could have spent more time developing other, more useful skills. When and if I have children I want to make sure they do not spend too much time with social media, games, and AI. I am assuming that AI will be used a lot more when I am older.